{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FreqDist(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        arr = []\n",
    "        for x in X:\n",
    "            vec = CountVectorizer()\n",
    "            freqs = vec.fit_transform([x]).toarray()\n",
    "            arr.append({\n",
    "                'feature_names': vec.get_feature_names_out(),\n",
    "                'frequencies': freqs\n",
    "            })\n",
    "        return arr\n",
    "\n",
    "def lexical_diversity(freq_dist):\n",
    "    return len(freq_dist['feature_names']) / freq_dist['frequencies'].sum()\n",
    "\n",
    "def repetition_score(freq_dist):\n",
    "    # Compute the frequency steepness - more steepness means more repetition\n",
    "    freqs = freq_dist['frequencies']\n",
    "    return (freqs.max() - freqs.min()) / freqs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('datasets/final_train.csv')\n",
    "test_df = pd.read_csv('datasets/final_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short(text):\n",
    "    return len(text) > 10\n",
    "\n",
    "train_df = train_df[train_df['text'].apply(filter_short)]\n",
    "test_df = test_df[test_df['text'].apply(filter_short)]\n",
    "\n",
    "X_train = train_df['text']\n",
    "y_train = train_df['label']\n",
    "X_test = test_df['text']\n",
    "y_test = test_df['label']\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model bake-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('freq_dist', FreqDist()),\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            ('lexical_diversity', FunctionTransformer(lambda x: [[lexical_diversity(freq_dist)] for freq_dist in x])),\n",
    "            ('repetition_score', FunctionTransformer(lambda x: [[repetition_score(freq_dist)] for freq_dist in x])),\n",
    "        ]\n",
    "    )),\n",
    "])\n",
    "\n",
    "candidates = [\n",
    "    #LogisticRegression(random_state=42),\n",
    "    #MultinomialNB(),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    #CalibratedClassifierCV(LinearSVC(random_state=42, dual='auto')),\n",
    "]\n",
    "\n",
    "models = {}\n",
    "for model in candidates:\n",
    "    pipeline.steps.append(('model', model))\n",
    "    visualizer = ClassificationReport(pipeline, classes=['human', 'auto'], support=True)\n",
    "    visualizer.fit(X_train, y_train)\n",
    "    visualizer.score(X_test, y_test)\n",
    "    visualizer.show()\n",
    "    models[model.__class__.__name__] = pipeline\n",
    "print(models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['DecisionTreeClassifier'].steps[-1][1].classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('autogen.pkl', 'wb') as f:\n",
    "    m = models['DecisionTreeClassifier'].steps[-1][1]\n",
    "    pickle.dump(m, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish to Ensign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "from pyensign.ensign import Ensign\n",
    "from pyensign.events import Event\n",
    "\n",
    "ensign = Ensign(cred_path='../.streamlit/config.json')\n",
    "model = models['DecisionTreeClassifier'].steps[-1][1]\n",
    "meta = {\n",
    "    'model_class': model.__class__.__name__,\n",
    "    'train_size': str(X_train.shape[0]),\n",
    "    'test_size': str(X_test.shape[0]),\n",
    "    'classes': json.dumps(['human', 'auto']),\n",
    "}\n",
    "event = Event(pickle.dumps(model), mimetype=\"application/python-pickle\", schema_name='sklearn-model', schema_version=\"0.1.0\", meta=meta)\n",
    "#await ensign.publish(\"autogen-model\", event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = await ensign.query(\"SELECT * FROM autogen-model\")\n",
    "async for message in cursor:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('freq_dist', FreqDist()),\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            ('lexical_diversity', FunctionTransformer(lambda x: [[lexical_diversity(freq_dist)] for freq_dist in x])),\n",
    "            ('repetition_score', FunctionTransformer(lambda x: [[repetition_score(freq_dist)] for freq_dist in x])),\n",
    "        ]\n",
    "    )),\n",
    "    #('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "model_data = pickle.loads(event.data)\n",
    "print(pipeline.transform(['This is some text']))\n",
    "pipeline.steps.append(('model', model_data))\n",
    "classes = json.loads(event.meta['classes'])\n",
    "classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
